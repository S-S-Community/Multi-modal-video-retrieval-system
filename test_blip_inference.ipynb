{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73250eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset \n",
    "import transformers\n",
    "from transformers import BlipProcessor, BlipForImageTextRetrieval,BlipForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Resize\n",
    "import os\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import shutil\n",
    "import json\n",
    "from PIL import Image\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3918ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Warming up model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "import time\n",
    "\n",
    "class FastSurveillanceBLIPCaptioner:\n",
    "    def __init__(self, model_path):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load YOUR fine-tuned model\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,  # This keeps CUDA + reduces memory\n",
    "    device_map=\"cuda\"           # Automatic CUDA placement\n",
    ")\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path)\n",
    "        self.model.to(self.device).eval()\n",
    "        \n",
    "        # Warmup\n",
    "        print(\"üî• Warming up model...\")\n",
    "        dummy = Image.new('RGB', (384, 384), 'red')\n",
    "        for _ in range(3):\n",
    "            self._fast_caption(dummy)\n",
    "        print(\"‚úÖ Ready!\")\n",
    "    \n",
    "    def _fast_caption(self, image):\n",
    "        inputs = self.processor(\n",
    "            images=image, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,  # This already contains pad_token_id\n",
    "                max_length=30,        # Good for surveillance detail\n",
    "                num_beams=1,         # Fast greedy decoding\n",
    "                do_sample=False,     \n",
    "                early_stopping=True\n",
    "                # Removed the duplicate pad_token_id!\n",
    "            )\n",
    "        \n",
    "        return self.processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def caption_single_image(self, image_path):\n",
    "        if isinstance(image_path, str):\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        else:\n",
    "            image = image_path\n",
    "            \n",
    "        # Resize for optimal speed/quality balance\n",
    "        image = image.resize((384, 384), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        start = time.time()\n",
    "        caption = self._fast_caption(image)\n",
    "        time_ms = (time.time() - start) * 1000\n",
    "        \n",
    "        return {\n",
    "            'caption': caption, \n",
    "            'inference_time_ms': time_ms\n",
    "        }\n",
    "\n",
    "# Test it:\n",
    "captioner = FastSurveillanceBLIPCaptioner(\"./blip_surveillance_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7337444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from bitsandbytes) (2.7.0+cu118)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!conda activate ai\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "408ea80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting inference speed test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1/4: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_2_f521bea5\\scene_frame0_1750044961.jpg -> \"the image is a full - scene screenshot showing an outdoor nighttime setting with a single bench visible in the foreground. there are no people or\" (0.3911s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 2/4: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_1_db28a20e\\scene_frame0_1750042287.jpg -> \"the image is a full - scene screenshot showing an outdoor nighttime scene with a street sign and traffic signs. there are no people or vehicles visible\" (0.3456s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 3/4: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id1_1750042287.jpg -> \"the image is a cropped object snapshot of a person standing in an outdoor area during the daytime. the person appears to be wearing a dark\" (0.3795s)\n",
      "Image 4/4: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id709_1750045634.jpg -> \"the image is a full - scene screenshot showing a man wearing a black t - shirt and white pants walking outdoors during the daytime. he appears\" (0.3406s)\n",
      "\n",
      "‚úÖ Test complete!\n",
      "Average inference time: 0.3642 seconds per image\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_inference_speed(captioner, image_paths):\n",
    "    \"\"\"Tests the inference speed of the captioner on a list of images.\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting inference speed test...\")\n",
    "    \n",
    "    total_time = 0\n",
    "    num_images = len(image_paths)\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        try:\n",
    "            # Use the captioner's built-in method\n",
    "            result = captioner.caption_single_image(image_path)\n",
    "            \n",
    "            caption = result['caption']\n",
    "            elapsed_time = result['inference_time_ms'] / 1000  # Convert to seconds\n",
    "            total_time += elapsed_time\n",
    "            \n",
    "            print(f\"Image {i+1}/{num_images}: {image_path} -> \\\"{caption}\\\" ({elapsed_time:.4f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            num_images -= 1 # Adjust count if an image fails\n",
    "            \n",
    "    if num_images > 0:\n",
    "        average_time = total_time / num_images\n",
    "        print(f\"\\n‚úÖ Test complete!\")\n",
    "        print(f\"Average inference time: {average_time:.4f} seconds per image\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No images were processed.\")\n",
    "\n",
    "# Create sample image paths with proper path formatting\n",
    "sample_images = [\n",
    "    r\"D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_2_f521bea5\\scene_frame0_1750044961.jpg\",\n",
    "    r\"D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_1_db28a20e\\scene_frame0_1750042287.jpg\",\n",
    "    r\"D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id1_1750042287.jpg\",\n",
    "    r\"D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id709_1750045634.jpg\"\n",
    "]\n",
    "\n",
    "# Run the test\n",
    "test_inference_speed(captioner, sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac165e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
