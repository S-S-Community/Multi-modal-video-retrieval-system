{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ea71b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 4080\n",
      "GPU memory: 17.2 GB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('=' * 50)\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'CUDA version: {torch.version.cuda}')\n",
    "print(f'GPU count: {torch.cuda.device_count()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU name: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c45d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch version: 2.7.0+cu118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch; print(f'Current PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef86568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4080\n",
      "VRAM: 16.0 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset \n",
    "import transformers\n",
    "from transformers import BlipProcessor, BlipForImageTextRetrieval,BlipForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Resize\n",
    "import os\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import shutil\n",
    "import json\n",
    "from PIL import Image\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "517229a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Root: D:\\keep\\htx\\fast_search\\output_snapshots\n",
      "Output Directory: ./blip_surveillance_finetuned\n",
      "Base Model: Salesforce/blip-image-captioning-base\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = r\"D:\\keep\\htx\\fast_search\\output_snapshots\"  # Your OUTPUT_SNAPSHOTS_PATH\n",
    "OUTPUT_DIR = \"./blip_surveillance_finetuned\"\n",
    "MODEL_NAME = \"Salesforce/blip-image-captioning-base\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset Root: {DATASET_ROOT}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Base Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6fc6e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Scanning dataset structure...\n",
      "Found 11 class folders: ['bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'chair', 'horse', 'motorbike', 'person', 'train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing class folders: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 51.51it/s]\n",
      "Processing class folders: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 51.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∏ Processing screenshots folder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing screenshot folders: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 214.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset loaded successfully!\n",
      "üìä Total samples: 1057\n",
      "üîç Object snapshots: 658\n",
      "üé¨ Full scenes: 399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_surveillance_dataframe(dataset_root):\n",
    "    \"\"\"\n",
    "    Convert surveillance dataset structure to DataFrame\n",
    "    \n",
    "    Your dataset structure:\n",
    "    - Class folders (bicycle, person, car, etc.) with cropped images + captions\n",
    "    - screenshots/ folder with scene images + captions\n",
    "    \"\"\"\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    print(\"üìÅ Scanning dataset structure...\")\n",
    "    \n",
    "    # Process class folders (object snapshots)\n",
    "    class_folders = [f for f in os.listdir(dataset_root) \n",
    "                    if os.path.isdir(os.path.join(dataset_root, f)) and f != \"screenshots\"]\n",
    "    \n",
    "    print(f\"Found {len(class_folders)} class folders: {class_folders}\")\n",
    "    \n",
    "    # Process object snapshots\n",
    "    for class_name in tqdm(class_folders, desc=\"Processing class folders\"):\n",
    "        class_path = os.path.join(dataset_root, class_name)\n",
    "        \n",
    "        # Get all image files in this class folder\n",
    "        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Look for corresponding caption file\n",
    "            caption_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "            caption_path = os.path.join(class_path, caption_file)\n",
    "            \n",
    "            if os.path.exists(caption_path):\n",
    "                try:\n",
    "                    # Read caption\n",
    "                    with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "                        caption = f.read().strip()\n",
    "                    \n",
    "                    # Verify image can be opened\n",
    "                    with Image.open(image_path) as img:\n",
    "                        width, height = img.size\n",
    "                    \n",
    "                    data_list.append({\n",
    "                        'image_path': image_path,\n",
    "                        'caption': caption,\n",
    "                        'image_type': 'object_snapshot',\n",
    "                        'class_name': class_name,\n",
    "                        'width': width,\n",
    "                        'height': height,\n",
    "                        'caption_length': len(caption.split())\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # Process screenshots folder\n",
    "    screenshots_path = os.path.join(dataset_root, \"screenshots\")\n",
    "    if os.path.exists(screenshots_path):\n",
    "        print(f\"\\nüì∏ Processing screenshots folder...\")\n",
    "        \n",
    "        # Get all subdirectories in screenshots\n",
    "        screenshot_dirs = [f for f in os.listdir(screenshots_path) \n",
    "                          if os.path.isdir(os.path.join(screenshots_path, f))]\n",
    "        \n",
    "        for screenshot_dir in tqdm(screenshot_dirs, desc=\"Processing screenshot folders\"):\n",
    "            screenshot_dir_path = os.path.join(screenshots_path, screenshot_dir)\n",
    "            \n",
    "            # Get all image files in this screenshot folder\n",
    "            image_files = [f for f in os.listdir(screenshot_dir_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(screenshot_dir_path, image_file)\n",
    "                \n",
    "                # Look for corresponding caption file\n",
    "                caption_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "                caption_path = os.path.join(screenshot_dir_path, caption_file)\n",
    "                \n",
    "                if os.path.exists(caption_path):\n",
    "                    try:\n",
    "                        # Read caption\n",
    "                        with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "                            caption = f.read().strip()\n",
    "                        \n",
    "                        # Verify image can be opened\n",
    "                        with Image.open(image_path) as img:\n",
    "                            width, height = img.size\n",
    "                        \n",
    "                        data_list.append({\n",
    "                            'image_path': image_path,\n",
    "                            'caption': caption,\n",
    "                            'image_type': 'full_scene',\n",
    "                            'class_name': 'scene',\n",
    "                            'width': width,\n",
    "                            'height': height,\n",
    "                            'caption_length': len(caption.split())\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {image_path}: {e}\")\n",
    "                        continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Total samples: {len(df)}\")\n",
    "    print(f\"üîç Object snapshots: {len(df[df['image_type'] == 'object_snapshot'])}\")\n",
    "    print(f\"üé¨ Full scenes: {len(df[df['image_type'] == 'full_scene'])}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = create_surveillance_dataframe(DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c083687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DATASET ANALYSIS\n",
      "==================================================\n",
      "Total samples: 1057\n",
      "Unique classes: 12\n",
      "Image types: {'object_snapshot': 658, 'full_scene': 399}\n",
      "\n",
      "Caption Length Statistics:\n",
      "Average caption length: 71.3 words\n",
      "Min caption length: 5 words\n",
      "Max caption length: 203 words\n",
      "\n",
      "Class Distribution:\n",
      "class_name\n",
      "scene        399\n",
      "person       398\n",
      "car          130\n",
      "chair         60\n",
      "bus           23\n",
      "bicycle       20\n",
      "bottle        15\n",
      "motorbike      6\n",
      "train          3\n",
      "bird           1\n",
      "boat           1\n",
      "horse          1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample Data:\n",
      "                                          image_path  \\\n",
      "0  D:\\keep\\htx\\fast_search\\output_snapshots\\bicyc...   \n",
      "1  D:\\keep\\htx\\fast_search\\output_snapshots\\bicyc...   \n",
      "2  D:\\keep\\htx\\fast_search\\output_snapshots\\bicyc...   \n",
      "3  D:\\keep\\htx\\fast_search\\output_snapshots\\bicyc...   \n",
      "4  D:\\keep\\htx\\fast_search\\output_snapshots\\bicyc...   \n",
      "\n",
      "                                             caption       image_type  \\\n",
      "0  The image shows a bicycle parked on what appea...  object_snapshot   \n",
      "1  The image is taken outdoors, showcasing a scen...  object_snapshot   \n",
      "2  A person is riding a blue bicycle on the sidew...  object_snapshot   \n",
      "3  The image shows a person riding a bicycle on w...  object_snapshot   \n",
      "4  This is a photograph of an individual riding a...  object_snapshot   \n",
      "\n",
      "  class_name  width  height  caption_length  \n",
      "0    bicycle    205     106              61  \n",
      "1    bicycle    179     338              14  \n",
      "2    bicycle    226     459              10  \n",
      "3    bicycle    287     332              77  \n",
      "4    bicycle    404     592              73  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"üìä DATASET ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique classes: {df['class_name'].nunique()}\")\n",
    "print(f\"Image types: {df['image_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# Caption length analysis\n",
    "print(f\"\\nCaption Length Statistics:\")\n",
    "print(f\"Average caption length: {df['caption_length'].mean():.1f} words\")\n",
    "print(f\"Min caption length: {df['caption_length'].min()} words\")\n",
    "print(f\"Max caption length: {df['caption_length'].max()} words\")\n",
    "\n",
    "# Class distribution\n",
    "print(f\"\\nClass Distribution:\")\n",
    "class_counts = df['class_name'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nSample Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Plot caption length distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['caption_length'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Caption Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Caption Length Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['image_type'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Image Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Image Type Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae6168c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Preprocessing surveillance data...\n",
      "Removed 805 samples with extreme caption lengths\n",
      "Remaining samples after cleaning: 252\n",
      "üîç Verifying image files exist...\n",
      "Removed 805 samples with extreme caption lengths\n",
      "Remaining samples after cleaning: 252\n",
      "üîç Verifying image files exist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 252/252 [00:00<00:00, 3604.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final dataset size: 252 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_surveillance_data(df):\n",
    "    \"\"\"Clean and preprocess the surveillance dataset\"\"\"\n",
    "    \n",
    "    print(\"üßπ Preprocessing surveillance data...\")\n",
    "    \n",
    "    # Remove samples with very short or very long captions\n",
    "    initial_count = len(df)\n",
    "    df = df[(df['caption_length'] >= 3) & (df['caption_length'] <= 50)]  # Shorter max for BLIP\n",
    "    print(f\"Removed {initial_count - len(df)} samples with extreme caption lengths\")\n",
    "    \n",
    "    # Remove samples with missing or empty captions\n",
    "    df = df[df['caption'].str.strip() != '']\n",
    "    print(f\"Remaining samples after cleaning: {len(df)}\")\n",
    "    \n",
    "    # Clean captions - keep them simple for BLIP\n",
    "    df['caption'] = df['caption'].str.strip()\n",
    "    \n",
    "    # Verify all image files exist\n",
    "    print(\"üîç Verifying image files exist...\")\n",
    "    valid_images = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Verifying images\"):\n",
    "        if os.path.exists(row['image_path']):\n",
    "            try:\n",
    "                with Image.open(row['image_path']) as img:\n",
    "                    # Verify image can be opened\n",
    "                    img.verify()\n",
    "                valid_images.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Invalid image: {row['image_path']} - {e}\")\n",
    "        else:\n",
    "            print(f\"Missing image: {row['image_path']}\")\n",
    "    \n",
    "    df = df.loc[valid_images].reset_index(drop=True)\n",
    "    print(f\"‚úÖ Final dataset size: {len(df)} samples\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess the data\n",
    "df = preprocess_surveillance_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02472d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data Split:\n",
      "Training samples: 160\n",
      "Validation samples: 41\n",
      "Test samples: 51\n",
      "\n",
      "Training set distribution:\n",
      "image_type\n",
      "object_snapshot    138\n",
      "full_scene          22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set distribution:\n",
      "image_type\n",
      "object_snapshot    35\n",
      "full_scene          6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df['image_type']\n",
    ")\n",
    "\n",
    "# Further split training data for validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['image_type']\n",
    ")\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(train_df['image_type'].value_counts())\n",
    "\n",
    "print(f\"\\nValidation set distribution:\")\n",
    "print(val_df['image_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2986717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurveillanceDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, max_length=512):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load and process image\n",
    "        try:\n",
    "            image = Image.open(row['image_path']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {row['image_path']}: {e}\")\n",
    "            # Return a dummy image if loading fails\n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "        \n",
    "        # Get caption (use original caption for BLIP fine-tuning)\n",
    "        caption = row['caption']\n",
    "        \n",
    "        # Process image and text for BLIP\n",
    "        inputs = self.processor(\n",
    "            image, \n",
    "            caption, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension and prepare for training\n",
    "        item = {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(),\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze()\n",
    "        }\n",
    "        \n",
    "        # For BLIP fine-tuning, labels are the same as input_ids\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8a6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737cc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW (Secure):\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Use environment variable\n",
    "hf_token = os.getenv('HUGGING_FACE_TOKEN')\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úÖ Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è HUGGING_FACE_TOKEN not found in environment variables\")\n",
    "    # Fallback: prompt for token\n",
    "    login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "08ba976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading BLIP model and processor...\n",
      "‚úÖ Model loaded successfully!\n",
      "Model parameters: 247,414,076\n",
      "Trainable parameters: 247,414,076\n",
      "‚úÖ Model loaded successfully!\n",
      "Model parameters: 247,414,076\n",
      "Trainable parameters: 247,414,076\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ Loading BLIP model and processor...\")\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "880e8714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Datasets created:\n",
      "Training dataset: 160 samples\n",
      "Validation dataset: 41 samples\n",
      "\n",
      "üîç Testing dataset loading...\n",
      "Sample keys: dict_keys(['pixel_values', 'input_ids', 'attention_mask', 'labels'])\n",
      "Input IDs shape: torch.Size([512])\n",
      "Pixel values shape: torch.Size([3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SurveillanceDataset(train_df, processor)\n",
    "val_dataset = SurveillanceDataset(val_df, processor)\n",
    "\n",
    "print(f\"üìö Datasets created:\")\n",
    "print(f\"Training dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# Test dataset loading\n",
    "print(\"\\nüîç Testing dataset loading...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Pixel values shape: {sample['pixel_values'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1886e5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x21fde390a90>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d23933ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e9a5ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 0.6280491948127747\n",
      "Epoch: 1\n",
      "Loss: 0.6280491948127747\n",
      "Epoch: 1\n",
      "Loss: 0.3049754500389099\n",
      "Epoch: 2\n",
      "Loss: 0.3049754500389099\n",
      "Epoch: 2\n",
      "Loss: 0.11615601927042007\n",
      "Epoch: 3\n",
      "Loss: 0.11615601927042007\n",
      "Epoch: 3\n",
      "Loss: 0.09997476637363434\n",
      "Epoch: 4\n",
      "Loss: 0.09997476637363434\n",
      "Epoch: 4\n",
      "Loss: 0.031169608235359192\n",
      "CPU times: total: 10.8 s\n",
      "Wall time: 58 s\n",
      "Loss: 0.031169608235359192\n",
      "CPU times: total: 10.8 s\n",
      "Wall time: 58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Start training \n",
    "\n",
    "for epoch in range(5):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "    attention_mask = batch.pop(\"attention_mask\").to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "  print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a58ac10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor saved to ./blip_surveillance_finetuned\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model and processor\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model and processor saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bca7c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## INSTALL PACKAGES #############\n",
    "\n",
    "# pip install transformers optimum[onnx] onnx onnxruntime\n",
    "# pip install torchvision pillow\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "###################### CODE TO EXPORT ###############\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from optimum.exporters.onnx import main_export\n",
    "from pathlib import Path\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(OUTPUT_DIR)\n",
    "model = BlipForConditionalGeneration.from_pretrained(OUTPUT_DIR)\n",
    "# Define model and export settings\n",
    "\n",
    "def compress_blip_dynamic_quantization(model_path, output_path):\n",
    "    \"\"\"\n",
    "    Easiest method: Dynamic INT8 quantization\n",
    "    - 60-75% size reduction\n",
    "    - 2-3x speed improvement\n",
    "    - Minimal accuracy loss (<2%)\n",
    "    - Works out of the box\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ Method 1: Dynamic Quantization\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load your fine-tuned model\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.float32)\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    \n",
    "    # Get original model size\n",
    "    original_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "    print(f\"Original model size: {original_size:.1f} MB\")\n",
    "    \n",
    "    # Dynamic quantization (THE MAGIC LINE!)\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, \n",
    "        {nn.Linear},  # Quantize all Linear layers\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    \n",
    "    # Get quantized model size\n",
    "    quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters()) / (1024**2)\n",
    "    print(f\"Quantized model size: {quantized_size:.1f} MB\")\n",
    "    print(f\"Size reduction: {((original_size - quantized_size) / original_size * 100):.1f}%\")\n",
    "    \n",
    "    # Save the quantized model\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    torch.save(quantized_model.state_dict(), os.path.join(output_path, \"quantized_model.pth\"))\n",
    "    torch.save(quantized_model, os.path.join(output_path, \"quantized_model_full.pth\"))\n",
    "    processor.save_pretrained(output_path)\n",
    "    \n",
    "    print(f\"‚úÖ Quantized model saved to: {output_path}\")\n",
    "    \n",
    "    return quantized_model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "025916b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Method 1: Dynamic Quantization\n",
      "==================================================\n",
      "Original model size: 943.8 MB\n",
      "Quantized model size: 95.4 MB\n",
      "Size reduction: 89.9%\n",
      "‚úÖ Quantized model saved to: ./blip_compressed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BlipForConditionalGeneration(\n",
       "   (vision_model): BlipVisionModel(\n",
       "     (embeddings): BlipVisionEmbeddings(\n",
       "       (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "     )\n",
       "     (encoder): BlipEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x BlipEncoderLayer(\n",
       "           (self_attn): BlipAttention(\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (qkv): DynamicQuantizedLinear(in_features=768, out_features=2304, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "             (projection): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "           )\n",
       "           (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): BlipMLP(\n",
       "             (activation_fn): GELUActivation()\n",
       "             (fc1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "             (fc2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "           )\n",
       "           (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (text_decoder): BlipTextLMHeadModel(\n",
       "     (bert): BlipTextModel(\n",
       "       (embeddings): BlipTextEmbeddings(\n",
       "         (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "         (position_embeddings): Embedding(512, 768)\n",
       "         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (encoder): BlipTextEncoder(\n",
       "         (layer): ModuleList(\n",
       "           (0-11): 12 x BlipTextLayer(\n",
       "             (attention): BlipTextAttention(\n",
       "               (self): BlipTextSelfAttention(\n",
       "                 (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "                 (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "                 (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "                 (dropout): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "               (output): BlipTextSelfOutput(\n",
       "                 (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (crossattention): BlipTextAttention(\n",
       "               (self): BlipTextSelfAttention(\n",
       "                 (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "                 (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "                 (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "                 (dropout): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "               (output): BlipTextSelfOutput(\n",
       "                 (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BlipTextIntermediate(\n",
       "               (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BlipTextOutput(\n",
       "               (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (cls): BlipTextOnlyMLMHead(\n",
       "       (predictions): BlipTextLMPredictionHead(\n",
       "         (transform): BlipTextPredictionHeadTransform(\n",
       "           (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "           (transform_act_fn): GELUActivation()\n",
       "           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "         )\n",
       "         (decoder): DynamicQuantizedLinear(in_features=768, out_features=30524, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " BlipProcessor:\n",
       " - image_processor: BlipImageProcessor {\n",
       "   \"do_convert_rgb\": true,\n",
       "   \"do_normalize\": true,\n",
       "   \"do_rescale\": true,\n",
       "   \"do_resize\": true,\n",
       "   \"image_mean\": [\n",
       "     0.48145466,\n",
       "     0.4578275,\n",
       "     0.40821073\n",
       "   ],\n",
       "   \"image_processor_type\": \"BlipImageProcessor\",\n",
       "   \"image_std\": [\n",
       "     0.26862954,\n",
       "     0.26130258,\n",
       "     0.27577711\n",
       "   ],\n",
       "   \"processor_class\": \"BlipProcessor\",\n",
       "   \"resample\": 3,\n",
       "   \"rescale_factor\": 0.00392156862745098,\n",
       "   \"size\": {\n",
       "     \"height\": 384,\n",
       "     \"width\": 384\n",
       "   }\n",
       " }\n",
       " \n",
       " - tokenizer: BertTokenizerFast(name_or_path='./blip_surveillance_finetuned', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " )\n",
       " \n",
       " {\n",
       "   \"processor_class\": \"BlipProcessor\"\n",
       " })"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_blip_dynamic_quantization(OUTPUT_DIR, \"./blip_compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2e88cc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è BetterTransformer not available, using FP16 only\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load your model in FP16 (CUDA compatible + 50% memory reduction!)\n",
    "model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"./blip_surveillance_finetuned\",\n",
    "    torch_dtype=torch.float32,  # This keeps CUDA + reduces memory\n",
    "    device_map=\"cuda\"           # Automatic CUDA placement\n",
    ")\n",
    "\n",
    "# Apply BetterTransformer optimization (20-40% speed boost)\n",
    "try:\n",
    "    model = model.to_bettertransformer()\n",
    "    print(\"‚úÖ BetterTransformer applied!\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è BetterTransformer not available, using FP16 only\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"./blip_surveillance_finetuned\")\n",
    "\n",
    "# Your model is now 2-3x faster, uses 50% less memory, and stays on CUDA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c8e43ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting inference speed test...\n",
      "Image 1/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id162_1750043191.jpg -> \"the image is a full - scene screenshot showing a motorcycle rider on a roadway. the rider appears to be wearing a helmet and protective gear, likely riding a motorcycle. the rider is wearing a helmet with a backpack and protective gear, likely for\" (0.8451s)\n",
      "Image 2/10: D:\\keep\\htx\\fast_search\\output_snapshots\\train\\train_id42_1750042847.jpg -> \"the image shows a green city bus on a street.\" (0.1670s)\n",
      "Image 1/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id162_1750043191.jpg -> \"the image is a full - scene screenshot showing a motorcycle rider on a roadway. the rider appears to be wearing a helmet and protective gear, likely riding a motorcycle. the rider is wearing a helmet with a backpack and protective gear, likely for\" (0.8451s)\n",
      "Image 2/10: D:\\keep\\htx\\fast_search\\output_snapshots\\train\\train_id42_1750042847.jpg -> \"the image shows a green city bus on a street.\" (0.1670s)\n",
      "Image 3/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_6_ece7a58b\\scene_frame5625_1750046921.jpg -> \"the image is a full - scene screenshot showing a street at night. there are no people or vehicles in this part of the image, possibly a street lamp, and a building in the background.\" (0.5564s)\n",
      "Image 3/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_6_ece7a58b\\scene_frame5625_1750046921.jpg -> \"the image is a full - scene screenshot showing a street at night. there are no people or vehicles in this part of the image, possibly a street lamp, and a building in the background.\" (0.5564s)\n",
      "Image 4/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id52_1750046773.jpg -> \"the image is a full - scene screenshot showing a man wearing a red shirt and shorts walking outdoors during the daytime. he appears to be dressed in casual attire, possibly in an urban setting.\" (0.5439s)\n",
      "Image 4/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id52_1750046773.jpg -> \"the image is a full - scene screenshot showing a man wearing a red shirt and shorts walking outdoors during the daytime. he appears to be dressed in casual attire, possibly in an urban setting.\" (0.5439s)\n",
      "Image 5/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_18_9cfdcff0\\scene_frame1125_1750044633.jpg -> \"the image is a color photograph of an outdoor parking lot at dusk. it features paved surfaces and a few greenery in the background. there are no people or vehicles visible in this section of the image.\" (0.5764s)\n",
      "Image 5/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_18_9cfdcff0\\scene_frame1125_1750044633.jpg -> \"the image is a color photograph of an outdoor parking lot at dusk. it features paved surfaces and a few greenery in the background. there are no people or vehicles visible in this section of the image.\" (0.5764s)\n",
      "Image 6/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id3_1750045357.jpg -> \"the image is a cropped object snapshot of a person standing in an outdoor area during the daytime. the person appears to be wearing a light - colored shirt and blue pants, and has a short haircut. there is no visible in the\" (0.6384s)\n",
      "Image 6/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id3_1750045357.jpg -> \"the image is a cropped object snapshot of a person standing in an outdoor area during the daytime. the person appears to be wearing a light - colored shirt and blue pants, and has a short haircut. there is no visible in the\" (0.6384s)\n",
      "Image 7/10: D:\\keep\\htx\\fast_search\\output_snapshots\\car\\car_id4_1750047534.jpg -> \"the image is a full - scene screenshot showing a silver sedan with a black license plate parked in a street scene. there are no visible people or additional context to describe.\" (0.4777s)\n",
      "Image 7/10: D:\\keep\\htx\\fast_search\\output_snapshots\\car\\car_id4_1750047534.jpg -> \"the image is a full - scene screenshot showing a silver sedan with a black license plate parked in a street scene. there are no visible people or additional context to describe.\" (0.4777s)\n",
      "Image 8/10: D:\\keep\\htx\\fast_search\\output_snapshots\\car\\car_id22_1750046673.jpg -> \"the image is a surveillance camera feed showing a car driving on a road during daylight hours. there are no visible people or additional context to describe within this particular context.\" (0.4526s)\n",
      "Image 8/10: D:\\keep\\htx\\fast_search\\output_snapshots\\car\\car_id22_1750046673.jpg -> \"the image is a surveillance camera feed showing a car driving on a road during daylight hours. there are no visible people or additional context to describe within this particular context.\" (0.4526s)\n",
      "Image 9/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id3_1750043008.jpg -> \"the image is blurry and lacks context to provide a detailed description.\" (0.2090s)\n",
      "Image 9/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id3_1750043008.jpg -> \"the image is blurry and lacks context to provide a detailed description.\" (0.2090s)\n",
      "Image 10/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_2_f521bea5\\scene_frame2250_1750045065.jpg -> \"the image is a full - scene screenshot showing an outdoor nighttime setting with a single bench visible in the foreground. there are no people or vehicles visible in this particular frame. the image is captured at night in an outdoor public space with a\" (0.6611s)\n",
      "\n",
      "‚úÖ Test complete!\n",
      "Average inference time: 0.5128 seconds per image\n",
      "Image 10/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_2_f521bea5\\scene_frame2250_1750045065.jpg -> \"the image is a full - scene screenshot showing an outdoor nighttime setting with a single bench visible in the foreground. there are no people or vehicles visible in this particular frame. the image is captured at night in an outdoor public space with a\" (0.6611s)\n",
      "\n",
      "‚úÖ Test complete!\n",
      "Average inference time: 0.5128 seconds per image\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"./blip_compressed\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"./blip_compressed\", torch_dtype=torch.float32)\n",
    "\n",
    "def test_inference_speed(model, processor, image_paths):\n",
    "    \"\"\"Tests the inference speed of the model on a list of images.\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting inference speed test...\")\n",
    "  \n",
    "\n",
    "    \n",
    "    total_time = 0\n",
    "    num_images = len(image_paths)\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Start timer\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Process image and generate caption\n",
    "            inputs = processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = inputs.pixel_values.to(device)\n",
    "            \n",
    "            generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "            generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            \n",
    "            # Stop timer\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate time for this image\n",
    "            elapsed_time = end_time - start_time\n",
    "            total_time += elapsed_time\n",
    "            \n",
    "            print(f\"Image {i+1}/{num_images}: {image_path} -> \\\"{generated_caption}\\\" ({elapsed_time:.4f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            num_images -= 1 # Adjust count if an image fails\n",
    "            \n",
    "    if num_images > 0:\n",
    "        average_time = total_time / num_images\n",
    "        print(f\"\\n‚úÖ Test complete!\")\n",
    "        print(f\"Average inference time: {average_time:.4f} seconds per image\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No images were processed.\")\n",
    "\n",
    "# # Get 10 sample image paths from the test set\n",
    "sample_images = test_df['image_path'].head(10).tolist()\n",
    "\n",
    "\n",
    "\n",
    "# # Run the test\n",
    "test_inference_speed(model, processor, sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4be77ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n",
      "TensorRT-LLM is not installed. Please install TensorRT-LLM or set TRTLLM_PLUGINS_PATH to the directory containing libnvinfer_plugin_tensorrt_llm.so to use converters for torch.distributed ops\n",
      "TensorRT-LLM is not installed. Please install TensorRT-LLM or set TRTLLM_PLUGINS_PATH to the directory containing libnvinfer_plugin_tensorrt_llm.so to use converters for torch.distributed ops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèéÔ∏è Method 4: TensorRT Optimization\n",
      "==================================================\n",
      "üîÑ Compiling with TensorRT (this may take a few minutes)...\n",
      "üîÑ Compiling with TensorRT (this may take a few minutes)...\n"
     ]
    },
    {
     "ename": "Unsupported",
     "evalue": "Observed exception\n  Explanation: Dynamo found no exception handler at the top-level compiled function when encountering an exception. Exception will propagate outside the compiled region.\n  Hint: Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled.\n  Hint: It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.\n\n  Developer debug context: raised exception ExceptionVariable(<class 'ValueError'>)\n\n\nfrom user code:\n   File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\models\\blip\\modeling_blip.py\", line 1004, in forward\n    outputs = self.text_decoder(\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupported\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Maximum CUDA performance achieved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trt_model, processor\n\u001b[1;32m---> 43\u001b[0m \u001b[43mcompress_blip_tensorrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[117], line 31\u001b[0m, in \u001b[0;36mcompress_blip_tensorrt\u001b[1;34m(model_path, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîÑ Compiling with TensorRT (this may take a few minutes)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Compile with TensorRT\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m trt_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_tensorrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43menabled_precisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkspace_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 1GB workspace\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ TensorRT compilation complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Maximum CUDA performance achieved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch_tensorrt\\_compile.py:286\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(module, ir, inputs, arg_inputs, kwarg_inputs, enabled_precisions, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m torchtrt_arg_inputs \u001b[38;5;241m=\u001b[39m prepare_inputs(arg_inputs)\n\u001b[0;32m    284\u001b[0m torchtrt_kwarg_inputs \u001b[38;5;241m=\u001b[39m prepare_inputs(kwarg_inputs)\n\u001b[1;32m--> 286\u001b[0m exp_program \u001b[38;5;241m=\u001b[39m dynamo_trace(\n\u001b[0;32m    287\u001b[0m     module, torchtrt_arg_inputs, kwarg_inputs\u001b[38;5;241m=\u001b[39mtorchtrt_kwarg_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    288\u001b[0m )\n\u001b[0;32m    289\u001b[0m trt_graph_module \u001b[38;5;241m=\u001b[39m dynamo_compile(\n\u001b[0;32m    290\u001b[0m     exp_program,\n\u001b[0;32m    291\u001b[0m     arg_inputs\u001b[38;5;241m=\u001b[39mtorchtrt_arg_inputs,\n\u001b[0;32m    292\u001b[0m     enabled_precisions\u001b[38;5;241m=\u001b[39menabled_precisions_set,\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    294\u001b[0m )\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trt_graph_module\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch_tensorrt\\dynamo\\_tracer.py:83\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(mod, inputs, arg_inputs, kwarg_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m dynamic_shapes \u001b[38;5;241m=\u001b[39m get_dynamic_shapes_args(mod, arg_inputs)\n\u001b[0;32m     82\u001b[0m dynamic_shapes\u001b[38;5;241m.\u001b[39mupdate(get_dynamic_shapes_kwargs(kwarg_inputs))\n\u001b[1;32m---> 83\u001b[0m exp_program \u001b[38;5;241m=\u001b[39m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch_arg_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exp_program\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\__init__.py:360\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExporting a ScriptModule is not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe try converting your ScriptModule to an ExportedProgram \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing `TS2EPConverter(mod, args, kwargs).convert()` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m     )\n\u001b[1;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\_trace.py:1092\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m         log_export_usage(\n\u001b[0;32m   1087\u001b[0m             event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport.error.unclassified\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1088\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39merror_type,\n\u001b[0;32m   1089\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m   1090\u001b[0m             flags\u001b[38;5;241m=\u001b[39m_EXPORT_FLAGS,\n\u001b[0;32m   1091\u001b[0m         )\n\u001b[1;32m-> 1092\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1094\u001b[0m     _EXPORT_FLAGS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\_trace.py:1065\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1064\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1065\u001b[0m     ep \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1066\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1067\u001b[0m     log_export_usage(\n\u001b[0;32m   1068\u001b[0m         event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport.time\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mend \u001b[38;5;241m-\u001b[39m start,\n\u001b[0;32m   1070\u001b[0m         flags\u001b[38;5;241m=\u001b[39m_EXPORT_FLAGS,\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mget_ep_stats(ep),\n\u001b[0;32m   1072\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\exported_program.py:121\u001b[0m, in \u001b[0;36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\_trace.py:2112\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, pre_dispatch, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace)\u001b[0m\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;66;03m# NOTE Export training IR rollout\u001b[39;00m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;66;03m# Old export calls export._trace(pre_dispatch=True)\u001b[39;00m\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# and there are still lot of internal/OSS callsites that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2109\u001b[0m \u001b[38;5;66;03m# export_training_ir_rollout_check returns True in OSS\u001b[39;00m\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# while internally it returns False UNLESS otherwise specified.\u001b[39;00m\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;129;01mand\u001b[39;00m export_training_ir_rollout_check():\n\u001b[1;32m-> 2112\u001b[0m     ep \u001b[38;5;241m=\u001b[39m \u001b[43m_export_for_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2114\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2120\u001b[0m     dtrace_structured(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexported_program\u001b[39m\u001b[38;5;124m\"\u001b[39m, payload_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mstr\u001b[39m(ep))\n\u001b[0;32m   2121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ep\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\_trace.py:1092\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m         log_export_usage(\n\u001b[0;32m   1087\u001b[0m             event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport.error.unclassified\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1088\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39merror_type,\n\u001b[0;32m   1089\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m   1090\u001b[0m             flags\u001b[38;5;241m=\u001b[39m_EXPORT_FLAGS,\n\u001b[0;32m   1091\u001b[0m         )\n\u001b[1;32m-> 1092\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1094\u001b[0m     _EXPORT_FLAGS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\_trace.py:1065\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1064\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1065\u001b[0m     ep \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1066\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1067\u001b[0m     log_export_usage(\n\u001b[0;32m   1068\u001b[0m         event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport.time\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mend \u001b[38;5;241m-\u001b[39m start,\n\u001b[0;32m   1070\u001b[0m         flags\u001b[38;5;241m=\u001b[39m_EXPORT_FLAGS,\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mget_ep_stats(ep),\n\u001b[0;32m   1072\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\exported_program.py:121\u001b[0m, in \u001b[0;36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\_trace.py:1975\u001b[0m, in \u001b[0;36m_export_for_training\u001b[1;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[0m\n\u001b[0;32m   1962\u001b[0m original_state_dict \u001b[38;5;241m=\u001b[39m _get_original_state_dict(mod)\n\u001b[0;32m   1964\u001b[0m export_func \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1965\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   1966\u001b[0m         _strict_export_lower_to_aten_ir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1973\u001b[0m     )\n\u001b[0;32m   1974\u001b[0m )\n\u001b[1;32m-> 1975\u001b[0m export_artifact \u001b[38;5;241m=\u001b[39m \u001b[43mexport_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[operator]\u001b[39;49;00m\n\u001b[0;32m   1976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1977\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1982\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1983\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_in_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_torch_jit_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1988\u001b[0m export_graph_signature \u001b[38;5;241m=\u001b[39m export_artifact\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39msig\n\u001b[0;32m   1990\u001b[0m forward_arg_names \u001b[38;5;241m=\u001b[39m _get_forward_arg_names(mod, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\_trace.py:1344\u001b[0m, in \u001b[0;36m_strict_export_lower_to_aten_ir\u001b[1;34m(mod, args, kwargs, dynamic_shapes, preserve_module_call_signature, pre_dispatch, original_state_dict, orig_in_spec, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace, lower_to_aten_callback)\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_strict_export_lower_to_aten_ir\u001b[39m(\n\u001b[0;32m   1332\u001b[0m     mod: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   1333\u001b[0m     args: \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     lower_to_aten_callback: Callable,\n\u001b[0;32m   1343\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ExportArtifact:\n\u001b[1;32m-> 1344\u001b[0m     gm_torch_level \u001b[38;5;241m=\u001b[39m \u001b[43m_export_to_torch_ir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrestore_fqn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't need to restore because we will do it later\u001b[39;49;00m\n\u001b[0;32m   1351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_log_export_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1355\u001b[0m     \u001b[38;5;66;03m# We detect the fake_mode by looking at gm_torch_level's placeholders, this is the fake_mode created in dynamo.\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m     (\n\u001b[0;32m   1357\u001b[0m         fake_args,\n\u001b[0;32m   1358\u001b[0m         fake_kwargs,\n\u001b[0;32m   1359\u001b[0m         dynamo_fake_mode,\n\u001b[0;32m   1360\u001b[0m     ) \u001b[38;5;241m=\u001b[39m _extract_fake_inputs(gm_torch_level, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\export\\_trace.py:739\u001b[0m, in \u001b[0;36m_export_to_torch_ir\u001b[1;34m(f, args, kwargs, dynamic_shapes, preserve_module_call_signature, disable_constraint_solver, allow_complex_guards_as_runtime_asserts, restore_fqn, _log_export_usage, same_signature)\u001b[0m\n\u001b[0;32m    735\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m _wrap_submodules(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    736\u001b[0m             f, preserve_module_call_signature, module_call_specs\n\u001b[0;32m    737\u001b[0m         )\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx, _ignore_backend_decomps():\n\u001b[1;32m--> 739\u001b[0m         gm_torch_level, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mexport(\n\u001b[0;32m    740\u001b[0m             f,\n\u001b[0;32m    741\u001b[0m             dynamic_shapes\u001b[38;5;241m=\u001b[39mdynamic_shapes,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    742\u001b[0m             assume_static_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    743\u001b[0m             tracing_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbolic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    744\u001b[0m             disable_constraint_solver\u001b[38;5;241m=\u001b[39mdisable_constraint_solver,\n\u001b[0;32m    745\u001b[0m             \u001b[38;5;66;03m# currently the following 2 flags are tied together for export purposes,\u001b[39;00m\n\u001b[0;32m    746\u001b[0m             \u001b[38;5;66;03m# but untangle for sake of dynamo export api\u001b[39;00m\n\u001b[0;32m    747\u001b[0m             prefer_deferred_runtime_asserts_over_guards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    748\u001b[0m             allow_complex_guards_as_runtime_asserts\u001b[38;5;241m=\u001b[39mallow_complex_guards_as_runtime_asserts,\n\u001b[0;32m    749\u001b[0m             _log_export_usage\u001b[38;5;241m=\u001b[39m_log_export_usage,\n\u001b[0;32m    750\u001b[0m             same_signature\u001b[38;5;241m=\u001b[39msame_signature,\n\u001b[0;32m    751\u001b[0m         )(\n\u001b[0;32m    752\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    753\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    754\u001b[0m         )\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ConstraintViolationError, ValueRangeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(UserErrorType\u001b[38;5;241m.\u001b[39mCONSTRAINT_VIOLATION, \u001b[38;5;28mstr\u001b[39m(e))  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1677\u001b[0m, in \u001b[0;36mexport.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;66;03m# TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1677\u001b[0m     result_traced \u001b[38;5;241m=\u001b[39m opt_f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConstraintViolationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1679\u001b[0m     constraint_violation_error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:659\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    658\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mremove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n",
      "\u001b[1;31mUnsupported\u001b[0m: Observed exception\n  Explanation: Dynamo found no exception handler at the top-level compiled function when encountering an exception. Exception will propagate outside the compiled region.\n  Hint: Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled.\n  Hint: It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.\n\n  Developer debug context: raised exception ExceptionVariable(<class 'ValueError'>)\n\n\nfrom user code:\n   File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\models\\blip\\modeling_blip.py\", line 1004, in forward\n    outputs = self.text_decoder(\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "import torch_tensorrt\n",
    "\n",
    "def compress_blip_tensorrt(model_path, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    TensorRT optimization for maximum CUDA performance\n",
    "    - 2-5x speed improvement\n",
    "    - Optimized for inference\n",
    "    - NVIDIA GPU specific\n",
    "    - Requires torch-tensorrt\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüèéÔ∏è Method 4: TensorRT Optimization\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "\n",
    "    # Load model\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    \n",
    "    # Create example inputs for TensorRT compilation\n",
    "    example_image = torch.randn(1, 3, 384, 384, dtype=torch.float16).to(device)\n",
    "    example_inputs = [example_image]\n",
    "    \n",
    "    print(\"üîÑ Compiling with TensorRT (this may take a few minutes)...\")\n",
    "    \n",
    "    # Compile with TensorRT\n",
    "    trt_model = torch_tensorrt.compile(\n",
    "        model,\n",
    "        inputs=example_inputs,\n",
    "        enabled_precisions={torch.float16},\n",
    "        workspace_size=1 << 30  # 1GB workspace\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ TensorRT compilation complete!\")\n",
    "    print(\"‚úÖ Maximum CUDA performance achieved\")\n",
    "    \n",
    "    return trt_model, processor\n",
    "\n",
    "compress_blip_tensorrt(OUTPUT_DIR, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "10a4c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"./blip_surveillance_finetuned\")\n",
    "processor = AutoProcessor.from_pretrained(\"./blip_surveillance_finetuned\")\n",
    "\n",
    "# Compress it (THE MAGIC LINE!)\n",
    "compressed_model = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save compressed model\n",
    "torch.save(compressed_model, \"./compressed_blip_ver_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ec8699e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üöÄ Testing inference speed for compressed_blip_ver_2.pth...\n",
      "üöÄ Starting inference speed test...\n",
      "Image 1/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id162_1750043191.jpg -> \"the image is a full - scene screenshot showing a motorcycle rider riding a motorcycle. the rider appears to be wearing a helmet and protective, likely\" (2.3915s)\n",
      "Image 1/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id162_1750043191.jpg -> \"the image is a full - scene screenshot showing a motorcycle rider riding a motorcycle. the rider appears to be wearing a helmet and protective, likely\" (2.3915s)\n",
      "Image 2/10: D:\\keep\\htx\\fast_search\\output_snapshots\\train\\train_id42_1750042847.jpg -> \"the image is a full - scene screenshot showing a city bus traveling on a street during the daytime. the bus has a distinctive logo on its side, which indicates it is a city bus.\" (2.0367s)\n",
      "Image 2/10: D:\\keep\\htx\\fast_search\\output_snapshots\\train\\train_id42_1750042847.jpg -> \"the image is a full - scene screenshot showing a city bus traveling on a street during the daytime. the bus has a distinctive logo on its side, which indicates it is a city bus.\" (2.0367s)\n",
      "Image 3/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_6_ece7a58b\\scene_frame5625_1750046921.jpg -> \"the image is a full - scene screenshot showing a nighttime scene of a street scene with a single street lamp in the background and a building in the background. there is no visible in this particular context to describe describe.\" (2.1953s)\n",
      "Image 3/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_6_ece7a58b\\scene_frame5625_1750046921.jpg -> \"the image is a full - scene screenshot showing a nighttime scene of a street scene with a single street lamp in the background and a building in the background. there is no visible in this particular context to describe describe.\" (2.1953s)\n",
      "Image 4/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id52_1750046773.jpg -> \"the image is a full - scene screenshot showing a man wearing a red shirt and shorts walking outdoors during the daytime.\" (1.3091s)\n",
      "Image 4/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id52_1750046773.jpg -> \"the image is a full - scene screenshot showing a man wearing a red shirt and shorts walking outdoors during the daytime.\" (1.3091s)\n",
      "Image 5/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_18_9cfdcff0\\scene_frame1125_1750044633.jpg -> \"the image is a color photograph of an outdoor parking lot with a paved area in the background.\" (1.0793s)\n",
      "Image 5/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_18_9cfdcff0\\scene_frame1125_1750044633.jpg -> \"the image is a color photograph of an outdoor parking lot with a paved area in the background.\" (1.0793s)\n",
      "Image 6/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id3_1750045357.jpg -> \"the image is a surveillance camera feed showing a person standing in an outdoor area during the daytime.\" (1.0417s)\n",
      "Image 6/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id3_1750045357.jpg -> \"the image is a surveillance camera feed showing a person standing in an outdoor area during the daytime.\" (1.0417s)\n",
      "Image 7/10: D:\\keep\\htx\\fast_search\\output_snapshots\\car\\car_id4_1750047534.jpg -> \"the image is a full - scene screenshot showing a car with a visible license plate. it appears to be in motion, likely in an outdoor public space or in an urban setting.\" (1.7983s)\n",
      "Image 7/10: D:\\keep\\htx\\fast_search\\output_snapshots\\car\\car_id4_1750047534.jpg -> \"the image is a full - scene screenshot showing a car with a visible license plate. it appears to be in motion, likely in an outdoor public space or in an urban setting.\" (1.7983s)\n",
      "Image 8/10: D:\\keep\\htx\\fast_search\\output_snapshots\\car\\car_id22_1750046673.jpg -> \"the image is a surveillance camera screenshot showing a car in motion on a road during the daytime.\" (1.4660s)\n",
      "Image 8/10: D:\\keep\\htx\\fast_search\\output_snapshots\\car\\car_id22_1750046673.jpg -> \"the image is a surveillance camera screenshot showing a car in motion on a road during the daytime.\" (1.4660s)\n",
      "Image 9/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id3_1750043008.jpg -> \"the image is a cropped object snapshot of a person standing in an outdoor area during the daytime. the person appears to be wearing a light - colored shirt and dark shorts, holding a phone in their hand.\" (2.2448s)\n",
      "Image 9/10: D:\\keep\\htx\\fast_search\\output_snapshots\\person\\person_id3_1750043008.jpg -> \"the image is a cropped object snapshot of a person standing in an outdoor area during the daytime. the person appears to be wearing a light - colored shirt and dark shorts, holding a phone in their hand.\" (2.2448s)\n",
      "Image 10/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_2_f521bea5\\scene_frame2250_1750045065.jpg -> \"the image is a full - scene screenshot showing an outdoor nighttime scene with a concrete bench and a street lamp in the background. the background is illuminated by a bright, there are no visible signs of any human activity or notable events in this particular\" (2.4586s)\n",
      "\n",
      "‚úÖ Test complete!\n",
      "Average inference time: 1.8021 seconds per image\n",
      "Image 10/10: D:\\keep\\htx\\fast_search\\output_snapshots\\screenshots\\screenshot_2_f521bea5\\scene_frame2250_1750045065.jpg -> \"the image is a full - scene screenshot showing an outdoor nighttime scene with a concrete bench and a street lamp in the background. the background is illuminated by a bright, there are no visible signs of any human activity or notable events in this particular\" (2.4586s)\n",
      "\n",
      "‚úÖ Test complete!\n",
      "Average inference time: 1.8021 seconds per image\n"
     ]
    }
   ],
   "source": [
    "# Load the newly compressed model\n",
    "# Note: The 'weights_only=False' is required for this version of torch.\n",
    "compressed_model_v2 = torch.load(\"./compressed_blip_ver_2.pth\", weights_only=False)\n",
    "\n",
    "# The 'processor' and 'sample_images' variables are already available from previous cells.\n",
    "# The 'test_inference_speed' function is also defined and handles CPU execution for quantized models.\n",
    "\n",
    "print(\"\\n\\nüöÄ Testing inference speed for compressed_blip_ver_2.pth...\")\n",
    "test_inference_speed(compressed_model_v2, processor, sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "091eed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\isaac\\AppData\\Local\\Temp\\ipykernel_45604\\2092227819.py\", line 8, in generate_caption\n",
      "    image = Image.fromarray(image)\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\PIL\\Image.py\", line 3304, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\gradio\\utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\isaac\\AppData\\Local\\Temp\\ipykernel_45604\\2092227819.py\", line 8, in generate_caption\n",
      "    image = Image.fromarray(image)\n",
      "  File \"c:\\Users\\isaac\\anaconda3\\envs\\ai\\lib\\site-packages\\PIL\\Image.py\", line 3304, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "processor = AutoProcessor.from_pretrained(OUTPUT_DIR)\n",
    "model = BlipForConditionalGeneration.from_pretrained(OUTPUT_DIR)\n",
    "model.to(device)\n",
    "# Define the prediction function\n",
    "def generate_caption(image):\n",
    "    # Process the image\n",
    "    image = Image.fromarray(image)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values.to(device)\n",
    "\n",
    "    # Generate caption\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_caption\n",
    "# Define the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=generate_caption,\n",
    "    inputs=gr.Image(),\n",
    "    outputs=gr.Textbox(),\n",
    "    live=True\n",
    ")\n",
    "# Launch the Gradio interface\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
